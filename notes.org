* Flow charts
  
** Optimize 
   #+BEGIN_SRC dot :file figs/opt.pdf
digraph {
prog, infile, outfile, pbsFile [shape=polygon,sides=4,skew=.4]
prog -> Optimize -> "prog.WriteInput" -> "WritePBS"
{ rank = same; "prog.WriteInput"; infile }
{ rank = same; "WritePBS"; pbsFile }
{ rank = same; "prog.ReadOut"; outfile }
infile -> "prog.WriteInput"
pbsFile -> WritePBS
WritePBS -> Submit
pbsFile -> Submit
outfile -> "prog.ReadOut"
Submit -> "prog.ReadOut"
a [shape = diamond, label = "err != nil"]
"prog.ReadOut" -> a
a -> return [label = "T"]
a -> "prog.ReadOut" [label = "F", tailport = "e", headport = "e"]
}
   #+END_SRC

   #+RESULTS:
   [[file:figs/opt.pdf]]

** Initialize
   #+BEGIN_SRC dot :file figs/init.pdf
      digraph {
ParseFlags -> ParseInfile -> WhichCluster
     }
   #+END_SRC

   #+RESULTS:
   [[file:figs/init.pdf]]
   
** Main
   #+BEGIN_SRC dot :file figs/main.pdf
  digraph {
  DoOpt, irdy [shape=diamond]
  ParseFlags -> ParseInfile
  ParseInfile -> WhichCluster
  WhichCluster -> LoadMolpro
  LoadMolpro -> DoOpt
  setup [label = "MakeDirs\nFormatZmat\nOptimize\nHandleOutput\nUpdateZmat\nFrequency"]
  DoOpt -> setup [label = "Yes"]
  geom [label = "Input[Geometry]"]
  DoOpt -> geom [label = "No"]
  setup -> LoadIntder
  geom -> LoadIntder
  LoadIntder -> irdy
  irdy -> ConvertCart [label = Yes]
  irdy -> "Fields(irdy)" [label = No]
  }
   #+END_SRC

   #+RESULTS:
   [[file:main.pdf]]
   
** New

#+BEGIN_SRC dot :file figs/cart.pdf
  digraph {
  a [label="Mkdirs\nOptimize\nFrequency"]
  b [label="Load geometry"]
  c [label = "Set up intder"]
  d [label = "Write pts intder\nRun intder\nBuild pts jobs\nSubmit pts jobs"]
  e [label = "Build pts w/o writing"]
  f [label = "Build cart jobs\nSubmit cart jobs"]
  g [label = "Drain jobs"]
  h [label = "GoCart"]
  i [label = "Get rel. energies"]
  { rank = same; d, e, f }
  DoOpt, DoPts, GoCart, h [shape=diamond]
  Start -> DoOpt
  DoOpt -> a [label = t]
  DoOpt -> b [label = f]
  a, b -> GoCart
  GoCart -> f [label = t]
  GoCart -> c [label=f]
  c -> DoPts
  DoPts -> d [label = t]
  DoPts -> e [label = f]
  d,e,f -> g
  g -> h
  h -> Spectro [label = t]
  h -> i [label = f]
  i -> Anpass -> Intder -> Spectro
  }

#+END_SRC

#+RESULTS:
[[file:figs/cart.pdf]]
   
* 7/28 fast results
  +---------+---------+---------+---------+---------+
  | Mp Harm | Id Harm | Sp Harm | Sp Fund | Sp Corr |
  +---------+---------+---------+---------+---------+
  |     0.0 |   945.8 |   945.8 |   933.8 |   933.8 |
  |     0.0 |   868.4 |   868.4 |   855.2 |   855.2 |
  |     0.0 |   766.3 |   766.3 |   754.0 |   754.0 |
  |     0.0 |   627.6 |   627.6 |   616.7 |   616.7 |
  |     0.0 |   608.3 |   608.3 |   604.0 |   603.7 |
  |     0.0 |   345.7 |   345.7 |   344.5 |   344.5 |
  +---------+---------+---------+---------+---------+


* For sure a mistake to run parallel without -j flag for number of jobs

* --progress writes to stderr apparently so should have used 2> but joblog updates realtime too
  - doesn't really seem to have more information than log besides the average time per job
    
* turned --progress back on, just log as much info as possible and see what is useful
  - vim :e ++ff=dos to handle dos line endings in prog file
  - or C-Q C-M to insert that character for find and replace
   
* Notes
  - main difference for go-cart is build points, I guess that makes go-cart a program?
    - kinda awkward since it uses molpro too
    - does that mean load gocart?
    - have fc arrays global but only initialize with make if go-cart
    - really just change go-cart derivative stuff to output molpro input file and that slots into queue
    - then work on the queue to limit number of jobs running at once
  - May want to recover [[file:main.go::cart,%20zmat,%20err%20=%20prog.HandleOutput("opt/opt")][from HandleOutput error]]
  - communicating goroutines between submit and readOut 
    - can't submit until some of the running ones finish so check between them
  - [[file:main.go::if%20err%20==%20ErrFileContainsError%20{][Error notes]]
    - TODO reremove blankoutput for sequoia
    - Removing this one too now since problem on Sequoia
    - same problem as below, solved by queue
    - || err == ErrBlankOutput { // ||
    - must be a better way to do this -> check queue
    - disable for now
    - (err == ErrFileNotFound && len(points) < pointsInit/20) {
    - write error found in case it can't be handled by resubmit
    - then we need to kill it, manually for now

* TODO convert build, submit, poll separate loops into concurrent build/submit, poll functions
  - build is fine on its own for small sets, but building larger jobs and numbers of jobs could be bad
  - some work on this already but maybe trying to do too much at once
  - just focus on replicating current functionality with channels between concurrent routines
    
* TODO handle numerical disps
  -
    // PROBLEM WITH NUMERICAL DISPS - 14 extra points in anpass not in intder
    // why the extra dummy atom in freqs intder too?  r2666=mason/hco+/freqs
    // this has been somewhat resolved, linear triatomics we take double
    // shortcut, only consider one of the bending modes and then only
    // calculate half of its points typically so either generate a full
    // intder file without the shortcuts or have to do these manual additions later

* TODO modularize and slot in go-cart as an option
  - Optimization is a step for SIC but assumed already done in go-cart
    - this doesnt have to be true, the geometry for go-cart has to be optimized at some point
    - add switch for optimizing with go-cart, for now assume no opt for it
  - Require molpro.in for go-cart as well instead of embedded template
    
* TODO make submit return job number for qstat checking
* TODO use qstat checking before resubmit
* TODO default input parameters 
  - probably before ParseInfile and then overwrite with what's present there
* TODO WhichCluster should probably be part of parseinfile
  - defaults should probably be part of that as well actually

* DONE how/when to handle num disps? 
** need to generate bottom of anpass.in after adding column to make work for hco/lin3atomics
   - non-problem, saves time for linear triatomics, but these are fast anyway
     - use freqs intder.in header for hco+ and I guess the other linears
   - have to use anp2int.awk to generate intder coordinates from an anpass file
   - also have to make sure anpass has the same number of variables as intder
     - ie degenerate x and y bends are treated as one in anpass the old way
       - and then duplicated in the final intder file manually
     - manual intervention required for now

* DONE Problem with sequoia freq associated with reading zmat from log file
  - it was reading the CCSD(t)-F12b energy line before the optimization finished
    - reporting job finished when it was still running
  - cannot replicate locally
  - potentially reading the log file before it's finished being written?
  - just skip freq if zmat is nil for now

* DONE need way to specify atom ordering in transition from molpro to intder
  - leave intder geometry in as template for this
  - sort by all fields in xyz coords to emulate what intder expects
  - problem randomly matching atom order to coordinates
    - H O O H for example if you flip the Os or Hs

** DONE if transform fails, try exchanging columns to fix it
   - molpro put my al2o2 in a different plane when setting one angle to 90.0
   - this broke the transform because the pattern didnt match
** WAIT also might need to be robust to slight variations in the coords
   - I think this is handled, but wait and see
   - ie not a perfect tie

* TODO resume from each point of the process
  - pts, freqs mainly; if opt fails need to restart and if freq fails just run that

* WAIT figure out a better way to handle templates
  - I think the current approach is okay - eventually shrink to only a molpro input file
    - the intder, anpass, and spectro should be generated
  - moved away from go templates but now using "template" input files
  - could bundle literals with the program and use others if found in the input file

* TODO use taylor.py internals to write anpass and intder files
  - only includes bottom of intder file, top falls under the hard one below
  - could write entire anpass from scratch though

* TODO automate internal coordinate generation                         :HARD:

* TODO replace intder, anpass, and spectro entirely                    :HARD:

* ErrFileNotFound brainstorming and eventual fix
ErrFileNotFound just means the file doesn't exist

valid reasons for no existence:
- parallel job submitted but in queue -> could check this with qstat on parallel jobid
- parallel has not run that job yet -> could check this from log file

reasons to resubmit:
- parallel job is running (confirmed by jobid)
- parallel log contains job => job ran but didn't write output
- parallel log does not contain job => job has not run yet or is running

so we may want to resubmit whether or not the log contains the job
- either it's currently running but taking forever || it ran but failed
I guess if it ran but failed we definitely want to resubmit
Is taking forever a good reason to resubmit?

to resubmit for taking forever:
- parallel job is running
- job is not in the log file

but these conditions are met by every job at the beginning <- herein lies the problem

even if you wait for one job to finish, all the other jobs meet this condition

how to differentiate between jobs that have yet to run and those that are taking too long or won't run

with --progress can see the number of jobs submitted
so if job is not in the log and #running < #numjobs some jobs have not been submitted

does this offer any additional insight?

identity of unsubmitted jobs is unknown as is when they may be submitted

is the "average completion time" part of the log different in the lingering file? could be something to check

as jobs fail to finish the average completion time does continue to increase

however, it starts at 0 and increases when the first jobs start to finish too

how to tell if job is stuck or just running?

if ErrFileNotFound && file is not in logfile && numjobs in prog is less than maxjobs, ie 3 < 8

file not in logfile => job is not finished == job is running or not started
numjobs < maxjobs in progfile => all jobs have been started => job is running
ErrFileNotFound => job is not actually running so we need to resubmit
