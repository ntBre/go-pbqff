* Counting points
  [[file:~/Projects/go/src/github.com/ntBre/misc/matrview/main.go::func main() {][See code here]]
  |------------+------------------------------+-------|
  | Derivative | Formula                      | Water |
  |------------+------------------------------+-------|
  |          2 | 2n^2                         |   162 |
  |          3 | (4n^3 + 6n^2 + 2n)/3         |  1140 |
  |          4 | (2n^4 + 4n^2)/3              |  4482 |
  |------------+------------------------------+-------|
  |      Total | (2n^4 + 4n^3 + 16n^2 + 2n)/3 |  5784 |
  |------------+------------------------------+-------|
  - with n = 3*natoms = ncoords
  - 162 total points for water 2d
  - 1302 total at 3d, but subtract 162 = 1140
  - 5784 total at 4d but subtract 1302 = 4482
** Answers Cart
  4d:
  (1) diagonal : n, 2 jobs each = 2n
  (2) 3,1 : n(n-1), 4 jobs each = 4n(n-1)
  (3) 2,1,1 : n(n-1)(n-2)/2, 8 jobs each = 4n(n-1)(n-2)
  (4) 2,2 : n(n-1)/2, 4 jobs each = 2n(n-1)
  (5) 1,1,1,1 : n(n-1)(n-2)(n-3)/24, 16 jobs each = 2/3n(n-1)(n-2)(n-3)
  
  3d:
  (1) diagonal : n, 4 jobs each = 4n
  (2) 2,1 : n(n-1), 6 jobs each = 6n(n-1)
  (3) 1,1,1 : n(n-1)(n-2)/6, 8 jobs each = 4/3n(n^2 - 3n + 2)
  
  2d:
  (1) diagonal : n, 2 jobs each = 2n
  (2) 1,1 : n(n-1)/2, 4 jobs each = 2n(n-1)
** Answers Grad, note calcs vs jobs
  4d:
  (1) diagonal : n, 4 calcs each = 4n
  (2) 2,1 : n(n-1), 6 calcs each = 6n(n-1)
  (3) 1,1,1 : n(n-1)(n-2)/6, 8 calcs each = 4/3n(n^2 + 3n + 2)
  
  3d:
  (1) diagonal : n, 3 calcs each = 3n
  (2) 1,1 : n(n-1)/2, 4 calcs each = 2n(n-1)
  
  2d:
  (1) all : n, 2 calcs each = 2n
** Work
  nxn 2d array for water

  1 
  1 2 
  1 2 3 
  1 2 3 4 
  1 2 3 4 5 
  1 2 3 4 5 6 
  1 2 3 4 5 6 7 
  1 2 3 4 5 6 7 8 
  1 2 3 4 5 6 7 8 9

  diagonal terms (i=j) require 2 calculations each
  off diagonal terms require 4 each
  there are n diagonal terms
  n^2 total terms so (n^2-n)/2 is the remaining since only need under diagonal

  so 2n+4*(n^2-n)/2 = 2n + 2n^2 - 2n = 2n^2 pts for seconds
  where n is the number of coordinates = 3*natoms

  for water that's 2*81 = 162, which sounds right

  nxnxn is hard to draw so just imagine it

  cases:
  (1) i = j = k -> all same
  (2) { i = j != k, i = k != j, j = k != i } -> two same, one different
  (3) i != j != k -> all different

  case (1) is the diagonal again, still n diagonal terms
  n^3 - n remaining then
  4 jobs per point in this case so 4n jobs

  case (2) - diagonal in 2d but not 3d
  also have to remember that we're under the 2d diagonal and 3d diagonal
  neglecting this we would have n^2 - n at each level, with n levels
  so n(n^2-n) without symmetry
  as we saw with the 2D case above, there are actually (n^2 - n)/2 at each level
  does the 1/2 assumption hold for the other diagonal? probably, so that gives
  n/2 * (n^2 - n)/2 = n*(n^2-n)/4 = (n^3-n^2)/4 for this case
  and there are 6 jobs per point
  so a total of 3(n^3 - n^2)/2 jobs

  (1) left us with n^3-n remaining, so the leftovers for (3) are
  n^3-n - (n^3-n^2)/4 = n^3 - n^3/4 -n + n^2/4 = 3n^3/4 + n^2/4 - n
  and there are 8 jobs per point for this case
  giving
  6n^3 + 2n^2 - 8n jobs
  adding these up gives

  4n + 3/2n^3 - 3/2n^2 + 6n^3 + 2n^2 - 8n 

  15/2n^3 + 1/2n^2 - 4n jobs total

  for h2o that gives

  15/2*9^3 + 1/2*81 - 36
  10935/2 + 81/2 - 36 = 5508 - 36 = 5472
  well that's not right
  forgot to divide by 2 in the final piece?


  (/2 for (3)) -> 3n^3 + n^2 - 4n
  adding again gives
  3/2n^3 - 3/2n^2 + 3n^3 + n^2
  9/2n^3 - 1/2n^2 = 3240 for h2o, still doesn't look right
  I think it's half this, probably need to divide by two for every dimension, so halve the last piece again

  (/4 for (3)) -> 3/2n^3 + 1/2n^2 - 2n
  4n + 3/2n^3 - 3/2n^2 + 3/2n^3 + 1/2n^2 - 2n
  this gives
  3n^3 - n^2 + 2n

* Flow charts
  
** Optimize 
   #+BEGIN_SRC dot :file figs/opt.pdf
digraph {
prog, infile, outfile, pbsFile [shape=polygon,sides=4,skew=.4]
prog -> Optimize -> "prog.WriteInput" -> "WritePBS"
{ rank = same; "prog.WriteInput"; infile }
{ rank = same; "WritePBS"; pbsFile }
{ rank = same; "prog.ReadOut"; outfile }
infile -> "prog.WriteInput"
pbsFile -> WritePBS
WritePBS -> Submit
pbsFile -> Submit
outfile -> "prog.ReadOut"
Submit -> "prog.ReadOut"
a [shape = diamond, label = "err != nil"]
"prog.ReadOut" -> a
a -> return [label = "T"]
a -> "prog.ReadOut" [label = "F", tailport = "e", headport = "e"]
}
   #+END_SRC

   #+RESULTS:
   [[file:figs/opt.pdf]]

** Initialize
   #+BEGIN_SRC dot :file figs/init.pdf
      digraph {
ParseFlags -> ParseInfile -> WhichCluster
     }
   #+END_SRC

   #+RESULTS:
   [[file:figs/init.pdf]]
   
** Main
   #+BEGIN_SRC dot :file figs/main.pdf
  digraph {
  DoOpt, irdy [shape=diamond]
  ParseFlags -> ParseInfile
  ParseInfile -> WhichCluster
  WhichCluster -> LoadMolpro
  LoadMolpro -> DoOpt
  setup [label = "MakeDirs\nFormatZmat\nOptimize\nHandleOutput\nUpdateZmat\nFrequency"]
  DoOpt -> setup [label = "Yes"]
  geom [label = "Input[Geometry]"]
  DoOpt -> geom [label = "No"]
  setup -> LoadIntder
  geom -> LoadIntder
  LoadIntder -> irdy
  irdy -> ConvertCart [label = Yes]
  irdy -> "Fields(irdy)" [label = No]
  }
   #+END_SRC

   #+RESULTS:
   [[file:main.pdf]]
   
** New

   #+BEGIN_SRC dot :file figs/cart.pdf
  digraph {
  a [label="Mkdirs\nOptimize\nFrequency"]
  b [label="Load geometry"]
  c [label = "Set up intder"]
  d [label = "Write pts intder\nRun intder\nBuild pts jobs\nSubmit pts jobs"]
  e [label = "Build pts w/o writing"]
  f [label = "Build cart jobs\nSubmit cart jobs"]
  g [label = "Drain jobs"]
  h [label = "GoCart"]
  i [label = "Get rel. energies"]
  { rank = same; d, e, f }
  DoOpt, DoPts, GoCart, h [shape=diamond]
  Start -> DoOpt
  DoOpt -> a [label = t]
  DoOpt -> b [label = f]
  a, b -> GoCart
  GoCart -> f [label = t]
  GoCart -> c [label=f]
  c -> DoPts
  DoPts -> d [label = t]
  DoPts -> e [label = f]
  d,e,f -> g
  g -> h
  h -> Spectro [label = t]
  h -> i [label = f]
  i -> Anpass -> Intder -> Spectro
  }

   #+END_SRC

   #+RESULTS:
   [[file:figs/cart.pdf]]
   
* 7/28 fast results
  +---------+---------+---------+---------+---------+
  | Mp Harm | Id Harm | Sp Harm | Sp Fund | Sp Corr |
  +---------+---------+---------+---------+---------+
  |     0.0 |   945.8 |   945.8 |   933.8 |   933.8 |
  |     0.0 |   868.4 |   868.4 |   855.2 |   855.2 |
  |     0.0 |   766.3 |   766.3 |   754.0 |   754.0 |
  |     0.0 |   627.6 |   627.6 |   616.7 |   616.7 |
  |     0.0 |   608.3 |   608.3 |   604.0 |   603.7 |
  |     0.0 |   345.7 |   345.7 |   344.5 |   344.5 |
  +---------+---------+---------+---------+---------+


* For sure a mistake to run parallel without -j flag for number of jobs

* --progress writes to stderr apparently so should have used 2> but joblog updates realtime too
  - doesn't really seem to have more information than log besides the average time per job
    
* turned --progress back on, just log as much info as possible and see what is useful
  - vim :e ++ff=dos to handle dos line endings in prog file
  - or C-Q C-M to insert that character for find and replace
   
* Notes
  - main difference for go-cart is build points, I guess that makes go-cart a program?
    - kinda awkward since it uses molpro too
    - does that mean load gocart?
    - have fc arrays global but only initialize with make if go-cart
    - really just change go-cart derivative stuff to output molpro input file and that slots into queue
    - then work on the queue to limit number of jobs running at once
  - May want to recover [[file:main.go::cart,%20zmat,%20err%20=%20prog.HandleOutput("opt/opt")][from HandleOutput error]]
  - communicating goroutines between submit and readOut 
    - can't submit until some of the running ones finish so check between them
  - [[file:main.go::if%20err%20==%20ErrFileContainsError%20{][Error notes]]
    - TODO reremove blankoutput for sequoia
    - Removing this one too now since problem on Sequoia
    - same problem as below, solved by queue
    - || err == ErrBlankOutput { // ||
    - must be a better way to do this -> check queue
    - disable for now
    - (err == ErrFileNotFound && len(points) < pointsInit/20) {
    - write error found in case it can't be handled by resubmit
    - then we need to kill it, manually for now

* TODO convert build, submit, poll separate loops into concurrent build/submit, poll functions
  - build is fine on its own for small sets, but building larger jobs and numbers of jobs could be bad
  - some work on this already but maybe trying to do too much at once
  - just focus on replicating current functionality with channels between concurrent routines
    
* TODO handle numerical disps
  -
    // PROBLEM WITH NUMERICAL DISPS - 14 extra points in anpass not in intder
    // why the extra dummy atom in freqs intder too?  r2666=mason/hco+/freqs
    // this has been somewhat resolved, linear triatomics we take double
    // shortcut, only consider one of the bending modes and then only
    // calculate half of its points typically so either generate a full
    // intder file without the shortcuts or have to do these manual additions later

* TODO modularize and slot in go-cart as an option
  - Optimization is a step for SIC but assumed already done in go-cart
    - this doesnt have to be true, the geometry for go-cart has to be optimized at some point
    - add switch for optimizing with go-cart, for now assume no opt for it
  - Require molpro.in for go-cart as well instead of embedded template
    
* TODO make submit return job number for qstat checking
* TODO use qstat checking before resubmit
* TODO default input parameters 
  - probably before ParseInfile and then overwrite with what's present there
* TODO WhichCluster should probably be part of parseinfile
  - defaults should probably be part of that as well actually

* DONE how/when to handle num disps? 
** need to generate bottom of anpass.in after adding column to make work for hco/lin3atomics
   - non-problem, saves time for linear triatomics, but these are fast anyway
     - use freqs intder.in header for hco+ and I guess the other linears
   - have to use anp2int.awk to generate intder coordinates from an anpass file
   - also have to make sure anpass has the same number of variables as intder
     - ie degenerate x and y bends are treated as one in anpass the old way
       - and then duplicated in the final intder file manually
     - manual intervention required for now

* DONE Problem with sequoia freq associated with reading zmat from log file
  - it was reading the CCSD(t)-F12b energy line before the optimization finished
    - reporting job finished when it was still running
  - cannot replicate locally
  - potentially reading the log file before it's finished being written?
  - just skip freq if zmat is nil for now

* DONE need way to specify atom ordering in transition from molpro to intder
  - leave intder geometry in as template for this
  - sort by all fields in xyz coords to emulate what intder expects
  - problem randomly matching atom order to coordinates
    - H O O H for example if you flip the Os or Hs

** DONE if transform fails, try exchanging columns to fix it
   - molpro put my al2o2 in a different plane when setting one angle to 90.0
   - this broke the transform because the pattern didnt match
** WAIT also might need to be robust to slight variations in the coords
   - I think this is handled, but wait and see
   - ie not a perfect tie

* TODO resume from each point of the process
  - pts, freqs mainly; if opt fails need to restart and if freq fails just run that

* WAIT figure out a better way to handle templates
  - I think the current approach is okay - eventually shrink to only a molpro input file
    - the intder, anpass, and spectro should be generated
  - moved away from go templates but now using "template" input files
  - could bundle literals with the program and use others if found in the input file

* TODO use taylor.py internals to write anpass and intder files
  - only includes bottom of intder file, top falls under the hard one below
  - could write entire anpass from scratch though

* TODO automate internal coordinate generation                         :HARD:

* TODO replace intder, anpass, and spectro entirely                    :HARD:

* ErrFileNotFound brainstorming and eventual fix
  ErrFileNotFound just means the file doesn't exist

  valid reasons for no existence:
  - parallel job submitted but in queue -> could check this with qstat on parallel jobid
  - parallel has not run that job yet -> could check this from log file

  reasons to resubmit:
  - parallel job is running (confirmed by jobid)
  - parallel log contains job => job ran but didn't write output
  - parallel log does not contain job => job has not run yet or is running

  so we may want to resubmit whether or not the log contains the job
  - either it's currently running but taking forever || it ran but failed
  I guess if it ran but failed we definitely want to resubmit
  Is taking forever a good reason to resubmit?

  to resubmit for taking forever:
  - parallel job is running
  - job is not in the log file

  but these conditions are met by every job at the beginning <- herein lies the problem

  even if you wait for one job to finish, all the other jobs meet this condition

  how to differentiate between jobs that have yet to run and those that are taking too long or won't run

  with --progress can see the number of jobs submitted
  so if job is not in the log and #running < #numjobs some jobs have not been submitted

  does this offer any additional insight?

  identity of unsubmitted jobs is unknown as is when they may be submitted

  is the "average completion time" part of the log different in the lingering file? could be something to check

  as jobs fail to finish the average completion time does continue to increase

  however, it starts at 0 and increases when the first jobs start to finish too

  how to tell if job is stuck or just running?

  if ErrFileNotFound && file is not in logfile && numjobs in prog is less than maxjobs, ie 3 < 8

  file not in logfile => job is not finished == job is running or not started
  numjobs < maxjobs in progfile => all jobs have been started => job is running
  ErrFileNotFound => job is not actually running so we need to resubmit
